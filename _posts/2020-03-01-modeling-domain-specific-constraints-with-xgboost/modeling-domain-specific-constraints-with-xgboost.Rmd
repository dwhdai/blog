---
title: "Modeling domain-specific constraints with xgboost"
description: |
Incorporating domain knowledge from subject matter experts is critical for building useful models. As statistician George Box said, *"All models are wrong, but some are useful."* How useful our models are depend critically on how well we apply domain knowledge to extract the signal from the noise.
author:
  - name: David Dai
    url: https://www.daviddai.ca/
    affiliation: Data Science & Advanced Analytics, Unity Health Toronto
    affiliation_url: https://www.chartdatascience.ca/david-dai
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    self_contained: false
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = F,
                      message = F)
```

# Current State

Recent methodological advances in conjunction with the wealth of available data has raised questions about the importance of subject matter experts and their domain expertise when it comes to building prediction models. The popularity of feature engineering, once considered of extreme critical importance in building the most performant prediction models, has taken a back-seat to give way to models like deep learning that put less emphasis on explicit user-defined features.

While this may be a viable strategy in certain use-cases (eg. big-tech companies that have a lot of data, intentionally collected for specific purposes, and can validate results prospectively using experimentation), the current state and foreseeable future of prediction modeling in healthcare and clinical medicine requires a more collaborative approach between the domain experts (doctors, nurses, clinical managers, hospital executives) and data analysts.

# 




# Misc

<blockquote>Given the rough
medical truism that serious diseases are less common than mild diseases, it is even likely that
underperformance in minority subsets could lead to disproportionate harm to patients. - Oakden-Rayner et al., NeurIPS 2019</blockquote>

With increased capabilities of modern day algorithms to extract signal from noisy data, as well as increased awareness of said algorithms, it is paramount that we not forget the purpose and art of model-building: to create a mathematical representation of *real life*. 


# A Simulated Experiment


```{r}
# Simulate some data
library(tidyverse)
set.seed(112892)

n <- 50000

gender <- rbinom(n = n, size = 1, prob = 0.5)
lvef_50 <- rbinom(n = n, size = 1, prob = 0.5)
warfarin <- rpois(lambda = 28, n = n)/4

inr <- 1 + 0.3*gender + 0*lvef_50 + 0.25*warfarin + 0.2*lvef_50*warfarin + 0.5*gender*lvef_50*warfarin + rnorm(mean = 0.25, n)

df <- tibble(
  gender = gender,
  lvef_50 = lvef_50,
  warfarin = warfarin,
  inr = inr
) %>% 
  # Create sampling weight for 
  mutate(sampling_weight = case_when(
    gender == 1 & lvef_50 == 1 & warfarin > 9 ~ 0.01,
    gender == 0 & lvef_50 == 1 & warfarin > 9~ 0.01,
    gender == 0 & lvef_50 == 1 & warfarin < 9 ~ 0.2,
    lvef_50 == 0 & warfarin < 9 ~ 0.01,
    TRUE ~ 1
  ))


set.seed(12)
bias_df <- df %>% 
  sample_frac(1,
              replace = TRUE,
              weight = sampling_weight) %>% 
  select(-sampling_weight, -lvef_50)
ggplot(data = bias_df %>% 
         slice(train_ind:valid_ind),
       aes(x = warfarin,
           y = inr)) + 
  stat_smooth() +
  geom_point() +
  facet_grid(gender ~ lvef_50)



```

```{r fit model}
library(xgboost)

train_ind <- 0.7*n
valid_ind <- 0.85*n

train <- bias_df %>% 
  slice(1:train_ind)

valid <- bias_df %>%
  slice((train_ind + 1):valid_ind)

test <- df %>% 
  select(-lvef_50, -sampling_weight) %>% 
  slice((valid_ind + 1):n)

train_mat <- xgb.DMatrix(train %>% 
                           select(-inr) %>% 
                           as.matrix(), 
                         label = train$inr)
valid_mat <- xgb.DMatrix(valid %>% 
                           select(-inr) %>% 
                           as.matrix(), 
                         label = valid$inr)
test_mat <- xgb.DMatrix(test %>% 
                          select(-inr) %>% 
                          as.matrix(), 
                        label = test$inr)

watchlist <- list(train = train_mat,
                  eval = valid_mat)

params <- list(max_depth = 3,
               eta = 0.03, 
               nthread = 4,
               eval_metric = "rmse",
               objective = "reg:squarederror")

xgb_fit <- xgb.train(data = train_mat,
                     params = params,
                     verbose = T,
                     nrounds = 1000,
                     early_stopping_rounds = 50,
                     watchlist = watchlist)

xgb_fit_monotonic <- xgb.train(data = train_mat,
                               params = params,
                               monotone_constraints = c(0,1),
                               verbose = T,
                               nrounds = 1000,
                               early_stopping_rounds = 50,
                               watchlist = watchlist)

shap_vals <- predict(xgb_fit, 
                     test_mat,
                     predcontrib = T) 

shap_vals_mono <- predict(xgb_fit_monotonic, 
                          test_mat,
                          predcontrib = T) 

```

```{r plots}
pred <- predict(xgb_fit_monotonic, test_mat)

ggplot(data = test %>%
         bind_cols(pred=pred),
       aes(x = warfarin,
           y = pred)) +
  # geom_point()
  stat_smooth()



plot(shap_vals[,2] ~ test$warfarin)
plot(shap_vals[,1] ~ test$gender)

xgb.plot.shap(data = test %>% 
                select(-inr) %>% 
                as.matrix(),
              shap_contrib = shap_vals, model = xgb_fit, top_n = 4)
```


# Conclusion


In reality, data analysis aims to elucidate the data generating mechanism, and in a world where the data are vast and noise is abundant, the understanding of subject domain and its quirks is crucial for creating a data analysis with utility. 